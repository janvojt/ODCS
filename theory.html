<html>
<head>
<link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
<h2 id="basicConcepts">Basic concepts for creating new Data Processing Units (DPUs) </h2>
        This section describes basic knowledge needed to prepare your own data processing units (DPUs) for ODCleanStore. 
        
        <h3>DPUs</h3>
        Data processing units typically consists of three parts:
        
        <ul>
            <li>the main executive part, which contains the method <code>execute()</code> being called when the DPU is executed as part of the pipeline execution. See an example for <a href="https://github.com/mff-uk/odcs/blob/master/module/SPARQL_transformer/src/main/java/cz/cuni/mff/xrg/odcs/transformer/SPARQL/SPARQLTransformer.java">SPARQL transformer</a> 
            	</li>
            <li>the configuration object, which configures main executive part and is used to persist the configuration dialog settings. See an example for <a href="https://github.com/mff-uk/odcs/blob/master/module/SPARQL_transformer/src/main/java/cz/cuni/mff/xrg/odcs/transformer/SPARQL/SPARQLTransformerConfig.java">SPARQL transformer</a></li>
            <li>the configuration dialog, which allows to configure the DPU in the graphical user interface of ODCleanStore. See an example for <a href="https://github.com/mff-uk/odcs/blob/master/module/SPARQL_transformer/src/main/java/cz/cuni/mff/xrg/odcs/transformer/SPARQL/SPARQLTransformerDialog.java">SPARQL transformer</a></li>
        </ul>
        Every DPU must implement only the main executive part. The other parts are optional.
        Based on the required features the DPU's programmer <a href="#whyUsePreImpl">should</a> use
        appropriate preimplemented class from package <code>cz.cuni.mff.xrg.odcs.commons.module.dpu</code>:
        <ul>
        	<li><code>ConfigurableBase</code> for DPU's with configuration and optionally with a configuration dialog.</li>
        	<li><code>NonConfigurableBase</code> for DPU's without configuration and dialog.</li>
        </ul>
        In order to provide configuration dialog the DPU's main class also has to implements interface 
			<div class="code"> <pre> <code>
interface cz.cuni.mff.xrg.odcs.commons.web.ConfigDialogProvider &lt;C extends DPUConfigObject&gt; </code> </pre> </div>         
        
        
        
        <h3>Types of DPUs</h3>
        <div>The application distinguished three logical types of DPUs: 
        <ul>
            <li>Extractor - DPU which extracts data from certain external data sources and produces outputs consumed by other DPUs. 
            	</li>
            <li>Transformer - DPU which transform locally available input data into output data.
            	The logical difference between extractor and transformer is that extractor downloads and triplifies external data, whereas transformer works with data being already locally available for the ODCleanStore engine. </li>
            <li>Loader - DPU which save data into the target file, database, etc.
                </li>
         </ul>
         The DPU, you will be creating, must be annotated with one of the annotations <code>@AsExtractor, @AsTransformer, @AsLoader</code> to denote that the DPU is of one of the types above.
        </div>

          
          <h3 id="secConfigurationConcept">DPU Configuration</h3>
		<div>Every DPU has a possibility to store configuration object that parametrizes its 
			execution. DPU developer may provide the configuration dialog for his DPU, so that user may edit the configuration object via the 
			dialog - configuration object is synchronized with the dialog. 
			<br/>
			
			The configuration object must implement interface
			<div class="code"> <pre> <code>
interface cz.cuni.mff.xrg.odcs.commons.configuration.DPUConfigObject </code> </pre> </div> 
			which is located in <code>commons</code> module. The interface requires implementation 
			of one method - <code>isValid</code> - which checks whether the configuration is valid when the user's input is stored from the configuration dialog to the configuration object.			</div>
            Configuration object may be also used to hold default configuration of the DPU by introducing default values for the required attributes. 
            <br/>Instead of implementing interface the configuration class can extend 
            following preimplemented configuration class. 
			<div class="code"> <pre> <code>
class cz.cuni.mff.xrg.odcs.commons.module.config.DPUConfigObject</code> </pre> </div> 
			This class provide default 'always true' implementation of - <code>isValid</code> -, 
			this implementation can be optionally override by programmer. 
			See <a href="#whyUsePreImpl">reasons</a> why the usage of preimplementad classes is a prefered solution. 
                        
         <h3>DPU Template</h3>
          Every DPU is associated with a default (template) configuration (such configuration may contain, e.g., a file from which data should be by default extracted or the SILK linker rule, which should be by default applied). DPU together with its default (template) configuration is called DPU template. 
             
          
            <h3>DPU Instance</h3>
          When DPU (e.g. RDF SPARQL Extractor, CSV Extractor, SILK Linker) is placed on the pipeline, such placement is called DPU instantiation and the result of such instantiation is called DPU instance. DPU instance has its own configuration being based on the template configuration of the DPU. DPU instance is always associated with the given pipeline. One pipeline may have more different DPU instances of the same DPU template. 
          
          <h3>Data Units</h3> <div>
		<p>Data between DPUs are passed in containers that are called 
		DataUnits. Therefore, data units can be comprehanded as data flow units, which describe data being passed between  DPUs. DPUs distinguish input and output data units. Input data units are data units which flow into the processed DPU, output data units are those flowing from the processed DPU.          
                Currently only RDF data units (units passing RDF data) are available. 
                </p>
                
                 <p>Data units cannot be created directly. To create DataUnit, introduce new <code>RDFDataUnit</code> <b>public</b> attribute of the created DPU and annotate it with <code>@OutputDataUnit</code> for output data unit and <code>@InputDataUnit</code> for input data unit. </p>
        
        <p>The following example demonstrate the functionality of adding new output data unit for the given DPU. By adding new data unit, you create new output data flow unit, which may be then filled with the data being outputted by that DPU. Every DPU may create one or more output data units.
<div class="code"> <pre> <code>
<span class="comment">// create output RDFDataUnit named "output"</span>
@OutputDataUnit(name="output")
public RDFDataUnit outputDataUnit;
</code> </pre> </div> </p>			
		 
		 
		 
		<p>The following example demonstrate the functionality of defining input data unit for DPU:
<div class="code"> <pre> <code>
<span class="comment">// prepare new input data unit </span><!--DataUnitList&lt;RDFDataUnit&gt; dataUnitList = RDFDataUnitList.create(context);-->
@InputDataUnit
public RDFDataUnit inputDataUnit;
</code> </pre> </div>	</p>	 
                
                
                <code>RDFDataUnitHelper</code> 
		provides methods for dealing with RDF content being passed in <code>RDFDataUnit</code> from one DPU to another DPU.  
		The selected methods of <code>RDFDataUnitHelper</code> interface, which may be called on  <code>RDFDataUnit</code>s by the DPU developer for processing content of data units or for filling the data units with new RDF data, are: 
		<ul>
                         <li> Methods for adding new RDF triples to Data Units
                        <ul>
                            <li>addTriple(Resource subject, URI predicate, Value object) - Add the defined triple (s,p,o) to the DataUnit.  </li>
          <li>addFromTurtleFile(File file) - Add triples from file to the data unit. The file type is Turtle (TTL) </li>
          <li>addFromRDFXMLFile(File file) - Add triples from file to the data unit. The file type is RDF/XML </li>
                        <li>addFromFile(File file, RDFFormat format) - Add triples from file to the data unit. The file type is
	 explicitely specified </li>
         
         <li>addFromSPARQLEndpoint(URL endpointURL, String namedGraph) - Add triples from SPARQL endpoint specified by the endpoint URL and named graph to the data unit. </li>
           <li>addFromSPARQLEndpoint(URL endpointURL, String namedGraph,String user, String password) - Add triples from SPARQL endpoint specified by the endpoint URL, named graph to the data unit, using user and password provided. </li>

                        </ul>
                        </li>  
                        
                        <li> Methods for retrieving RDF triples from Data Unit
                        <ul>
                            <li>List<Statement> getTriples() - Get list of all RDF triples (Statements) in DataUnit.</li>
                            <li>storeToFile(File file, RDFFormatType formatType)  - Saves triples from data unit to file. The file type is
	 explicitely specified. </li>
                            <li>executeSelectQuery(String selectQuery) - Execute SPARQL select query over RDF data in DataUnit.</li>
                            <li>executeConstructQuery(String selectQuery) - Execute SPARQL construct query over RDF data in DataUnit.</li>
                            
                        </ul>
                        </li>  
                        
                        <li> Methods for transforming RDF data via queries
                        <ul>
                            <li>transform(String updateQuery) - Transform RDF data in DataUnit using SPARQL (update) query </li>
                        </ul>
                        </li>

                        </ul>
                        </li>
                        
                        
		</ul> For detailed information about the available methods please refer to the 
		<span class="inCode">RDFDataUnit</span> class </div> </p>
		                
        <h3 id="context">Pipeline execution - DPU Context</h3> <div>
        <p>Each DPU has its execution context. The main interface is 
		<code>DPUContext</code>. 
            
        <p> Context provides access to the external environment of the DPU, such as to the working directories of DPUs. There are three places where DPU can store it's temporary files needed for its execution.
		<ul>
			<li><b>working directory</b> - This directory is unique for single DPU instance (DPU placement on the pipeline) and the particular run of the pipeline. Only currently running DPUs can see such directory, which is private for the given DPU instance and pipeline run. Working directory may be obtained by calling <code>context.getWorkingDir();</code> </li>
			<li><b>user directory</b> - This directory is unique for every DPU and the user who executes the DPU. Therefore, you can store DPU's user related data here; such data is visible to all instances od that DPU executed by that user. Proper concurency mechanisms is needed, because user directory is shared. User directory may be obtained by calling <span class="code">context.getUserDirectory();</span> </li>
			<li><b>global directory</b> - This directory is unique for a single DPU. It is
			shared by different DPU instances, pipeline runs, and users. You can store files that
			should be visible to your DPU at any time, such as certain caching mechanisms. Proper concurency mechanisms is needed, because global directory is shared. Global directory may be obtained by calling <code>context.getGlobalDirectory();</code> </li>
		</ul> 
                
                <p>
		These directories can be obtained by calling specific context's methods. Alternatively, you can use <b>FileManager</b> helper class.
		Let's demonstrate it's functionality on few examples.The following example demonstrates how to get path to the file in 
		the working	directory with the use of File Manager: 
<div class="code"> <pre> <code>
<span class="comment">// creates file manager</span>
FileManager fileManager = new FileManager(context);
<span class="comment">// obtains file in working directory</span>
File filePath = fileManager.getWorking().file("myFile.txt");
</code> </pre> </div></p>

                <p>
		The following example demonstrates how to get the path to the file in 
		the global sub-directory. The directory is automatically created.
<div class="code"> <pre> <code>		 
<span class="comment">// creates file manager</span>
FileManager fileManager = new FileManager(context);
<span class="comment">// obtains file in sub-directory in global directory</span>
File filePath = fileManager.getGlobal().directory("myDir").file("myFile.txt");
</code> </pre> </div> </p>		 
		 
		</div>
        
        
        <p>The DPU context provides ability 
		to send events of the DPU as it is executed. Such messages (events) are stored and accessible to the user via execution monitor in the administration interface of ODCleanStore. 
                 <p>
                     The following example demonstrates how the context may send various messages about the execution of the DPU.
<div class="code"> <pre> <code>		 
<span class="comment">// sends error message</span>
context.sendMessage(MessageType.ERROR, "MalformedURLException: "
					+ ex.getMessage());
</code> </pre> </div> </p></li>
           
       There are
        two ways how to report that the execution of the DPU has failed:
        <ol>
        	<li>Send ERROR message via DPU context. In such case the execution
        		normally continues and it is finished when <code>execute(DPUContext context)</code> ends.
        		But no mather how the execution continues, the pipeline execution is consider to finished with error. 
        	<li>By throwing exception durring execution of - <code>execute(DPUContext context)</code> -. 
        		Best is to use <code>DPUException</code> with reasonable message that describes the reason
        		of execution's failure.</li>
        </ol>
        
        Also in some cases the termination of the whole pipeline without failure can be required. As an example: we have
        an extractor DPU that harvest new data from some webpage. If there are not new data, there is no meaning in further 
        pipeline processing, but this does not mean that the execution failed.<br/>
        For such case the DPU can use <code>MessageType.TERMINATION_REQUEST</code> as message type. If this 
        message is send then the pipeline execution terminate after the current DPU. <br/>
        
        If both ERROR and TERMINATION_REQUEST are send, then TERMINATION_REQUEST is ignored.
        
        As the DPU's execution can be time consuming, ODCleanStore provides user with the possibility to 
        stop the execution. Unfortunately this can not be done without DPU's cooperation. 
        In order to cooperate, every DPU which should suppert its cancellation should periodically call <code>DPUContext.canceled()</code> 
        function. If this function returns true, the execution is stopped as soon as possible. If the
        execution is stopped on the user's request then after the ends of  <code>execute(DPUContext context)</code> -
        function thee <code>cleanUp()</code> method is called. The <code>cleanUp()</code> 
        method should take care of releasing all resources allocated by the DPU.
		
		
		
		<h3>Logging the DPU's activity</h3>
		<div>Every DPU should log extensively during its execution. To log in your DPU, use <code>slf4j</code>. 
			The log for each DPU is stored and accessible to the user by the 
			graphical user interface of ODCleanStore.  
			If you are not familiar with <code>slf4j</code>, the following example 
			shows the basics:<br/>
			<div class="code"> <pre> <code>
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MyDPU  {
	private final log = LoggerFactory.getLogger(MyDPU.class);
...
			</code> </pre> </div>
			
			<div class="code"> 
			<pre> <code>log.debug("Some message .. ");</code> <br/> <code>log.info("Some message .. ");</code> <br/> <code>log.warn("Some message .. ");</code> <br/> <code>log.error("Some message .. ");</code></pre> </div>
		</div>
	
	As you can see, there are various levels of logged messages. Basic log levels are DEBUG, INFO, WARNING, ERROR. If you run pipeline, by default WARNING+ logs are shown in the execution monitor.
	If you debug pipeline, all log levels are shown otherwise only INFO+ log are shown. If WARNING+ log is logged and
	there is no error during whole pipeline execution the execution will finished with state FINISHED_WARNING. 
	In difference to the DPU's messages the execution can never failed because of logged log.
    
	<h2 id="basicTechs">Basic Technologies</h2>

	<h3 id="osgi">OSGi bundles</h3>
        <p>The DPUs developed are provided to the ODCleanStore in the form of OSGi bundles. The OSGI can be view as the dependency management system.
Bundle is a jar file with additional informations in MANIFEST.MF. The advantage of using OSGi bundles instead of simple JAR archives is that OSGi supports dynamic loadings of JAR files with their dependency resolving - OSGI uses information in jar's MANIFEST.MF to identify the dependencies of the bundle. Also every DPU may use its own set of dependencies (JAR libraries), which are hidden to other DPUs, thus, two DPUs may use two conflicting dependencies at once. </p>  
        
        <p> Each OSGi bundle contains <code>maninifest.mf</code> file located in the bundle, <code>META-INF</code> folder. This files contains bundle description. The most important part contains settings for exported and imported packages. The Export-Package is the list of packages provided by the bundle
    while the Import-Package is the list of packages that the bundle requires from 
    others to export in order to work properly. Sample of this part is as follows: 
	
	
<div class="maven-pom"> <pre> <code>
Export-Package: cz.cuni.mff.xrg.odcs.loader.rdf
Import-Package: com.vaadin.data,com.vaadin.data.util,com.vaadin.server,c
 om.vaadin.shared.ui.combobox,com.vaadin.ui,cz.cuni.mff.xrg.odcs.commons.c
 onfiguration;version="0.0.1",cz.cuni.mff.xrg.odcs.commons.data;version="0
 .0.1",cz.cuni.mff.xrg.odcs.commons.loader;version="0.0.1",cz.cuni.xrg.int
 lib.commons.module.data;version="0.0.1",cz.cuni.mff.xrg.odcs.commons.modu
 le.dialog;version="0.0.1",cz.cuni.mff.xrg.odcs.commons.module.dpu;version
 ="0.0.1",cz.cuni.mff.xrg.odcs.commons.web;version="0.0.1",cz.cuni.xrg.int
 lib.rdf.enums,cz.cuni.mff.xrg.odcs.rdf.exceptions,cz.cuni.mff.xrg.odcs.rdf.
 interfaces
</code> </pre> </div>

<p> The <code>Export-Package</code> configuration option specifies packages that the DPU bundle provides to the external environment (ODCleanStore). On the other hand 
	<code>Import-Package</code> configuration option lists dependencies of the bundle -- the packages that the bundle requires from 
    others. As a result, such dependencies MUST be provide by the external environment (ODCleanStore) before the DPU is used. Dependencies are specified as packages, not JAR archives or classes in the  <code>maninifest.mf</code> file. </p>
        
        Manifest file  <code>maninifest.mf</code> also contains lines with <code>DPU-MainClass</code> and
	<code>DPU-Package</code> that are essential for bundle use. <code>DPU-MainClass</code> contains name of the main class of the DPU which is called when the DPU is executed on the pipeline. <code>DPU-Package</code> contains name of the package in which the main class is located.
        
        <h3>Maven</h3>
        <p><a href="http://maven.apache.org/">Apache Maven</a> is a software project management and comprehension tool. 
        The created DPUs are suggested to be maven projects, so that DPU bundles can be easily managed and created. In order to set up maven in your system, please follow <a href="http://maven.apache.org/run-maven/index.html">this site</a> </p>
        
        <p>TODO describe the minimum install - local repository </p>
        <p>TODO describe the work with dependencies, scopes - compile, .. </p>
            

	<p><code>Pom.xml</code> file is the basic configuration file needed when the maven project is being build. If you want to add dependency
	to the project (DPU bundle),  <code>pom.xml</code>  is the right place. See the sample <code>pom.xml</code> for <a href="https://github.com/mff-uk/odcs/blob/master/module/SPARQL_transformer/pom.xml">SPARQL transformer</a> </p>
        
    <h3 id="whyUsePreImpl">Why you should use preimplemented abstract classes and not interfaces, if these classes fit your needs?</h3>    
    The preimplemented classes contains default implementation for optionally implemented methods. 
    This is why their usage can made DPU's code simpler and also allows DPU's developer to focus
    directly on implementation of DPU's executive functionality. <br/>
	The other great advantage is easier switch to the newer ODCleanStore version. If a new 
	method will be introduced in the base interface then if possible default implementation
	will be provided in respective preimplements class. Thanks to that the DPU's 
	code itself does not have to change. The DPU can be directly recompiled with 
	latest ODCleanStore version and it's readily to be used.  

</body>
</html>